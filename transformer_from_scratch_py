# transformer_from_scratch.py
"""
Implementasi arsitektur Decoder-Only Transformer (GPT-style) dari nol
hanya menggunakan NumPy.
"""

import numpy as np

# =============================================================================
# 1. Helper Functions
# =============================================================================

def softmax(x):
    """Menghitung softmax secara stabil untuk menghindari masalah numerik."""
    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
    return e_x / np.sum(e_x, axis=-1, keepdims=True)

def gelu(x):
    """Aktivasi Gaussian Error Linear Unit (GELU)."""
    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))

# =============================================================================
# 2. Komponen Arsitektur Transformer
# =============================================================================

class PositionalEncoding:
    """Menambahkan informasi posisi ke dalam embedding input."""
    def __init__(self, d_model, max_len=5000):
        pe = np.zeros((max_len, d_model))
        position = np.arange(0, max_len, dtype=np.float32).reshape(-1, 1)
        div_term = np.exp(np.arange(0, d_model, 2).astype(np.float32) * -(np.log(10000.0) / d_model))
        
        pe[:, 0::2] = np.sin(position * div_term)
        pe[:, 1::2] = np.cos(position * div_term)
        
        self.pe = pe

    def __call__(self, x):
        """x memiliki shape: [batch_size, seq_len, d_model]"""
        return x + self.pe[:x.shape[1], :]

class LayerNormalization:
    """Implementasi Layer Normalization."""
    def __init__(self, d_model, eps=1e-5):
        self.eps = eps
        self.gamma = np.ones(d_model)
        self.beta = np.zeros(d_model)

    def __call__(self, x):
        mean = np.mean(x, axis=-1, keepdims=True)
        std = np.std(x, axis=-1, keepdims=True)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta

def scaled_dot_product_attention(q, k, v, mask=None):
    """Implementasi Scaled Dot-Product Attention."""
    d_k = q.shape[-1]
    scores = (q @ np.transpose(k, (0, 1, 3, 2))) / np.sqrt(d_k)
    
    if mask is not None:
        scores += mask
    
    attention_weights = softmax(scores)
    output = attention_weights @ v
    return output, attention_weights

class MultiHeadAttention:
    """Implementasi Multi-Head Attention."""
    def __init__(self, d_model, n_heads):
        self.n_heads = n_heads
        self.d_model = d_model
        assert d_model % n_heads == 0, "d_model harus bisa dibagi dengan n_heads"
        self.d_k = d_model // n_heads

        # Inisialisasi bobot (matriks proyeksi) secara acak
        self.W_q = np.random.randn(d_model, d_model)
        self.W_k = np.random.randn(d_model, d_model)
        self.W_v = np.random.randn(d_model, d_model)
        self.W_o = np.random.randn(d_model, d_model)
        
    def __call__(self, q, k, v, mask=None):
        batch_size = q.shape[0]

        q = q @ self.W_q
        k = k @ self.W_k
        v = v @ self.W_v

        q = q.reshape(batch_size, -1, self.n_heads, self.d_k).transpose(0, 2, 1, 3)
        k = k.reshape(batch_size, -1, self.n_heads, self.d_k).transpose(0, 2, 1, 3)
        v = v.reshape(batch_size, -1, self.n_heads, self.d_k).transpose(0, 2, 1, 3)

        x, self.attention_weights = scaled_dot_product_attention(q, k, v, mask)

        x = x.transpose(0, 2, 1, 3).reshape(batch_size, -1, self.d_model)
        output = x @ self.W_o
        return output

class PositionwiseFeedForward:
    """Implementasi FFN dua lapisan dengan aktivasi non-linear."""
    def __init__(self, d_model, d_ff):
        self.W_1 = np.random.randn(d_model, d_ff)
        self.b_1 = np.zeros(d_ff)
        self.W_2 = np.random.randn(d_ff, d_model)
        self.b_2 = np.zeros(d_model)

    def __call__(self, x):
        x = gelu(x @ self.W_1 + self.b_1)
        x = x @ self.W_2 + self.b_2
        return x

class DecoderBlock:
    """Menggabungkan semua komponen menjadi satu blok Decoder Transformer."""
    def __init__(self, d_model, n_heads, d_ff):
        self.mha = MultiHeadAttention(d_model, n_heads)
        self.ffn = PositionwiseFeedForward(d_model, d_ff)
        self.ln1 = LayerNormalization(d_model)
        self.ln2 = LayerNormalization(d_model)

    def __call__(self, x, mask):
        # Struktur Pre-Norm: Norm -> Sublayer -> Add (Residual Connection)
        attn_output = self.mha(self.ln1(x), self.ln1(x), self.ln1(x), mask)
        x = x + attn_output
        ffn_output = self.ffn(self.ln2(x))
        x = x + ffn_output
        return x

class GPTDecoder:
    """Arsitektur utama GPT-style Transformer (Decoder-Only)."""
    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, max_len=5000):
        # Komponen 1: Token Embedding
        self.token_embedding = np.random.randn(vocab_size, d_model)
        
        self.pos_encoder = PositionalEncoding(d_model, max_len)
        self.decoder_blocks = [DecoderBlock(d_model, n_heads, d_ff) for _ in range(n_layers)]
        self.ln_final = LayerNormalization(d_model)
        
        # Komponen 8: Output Layer
        self.output_projection = np.random.randn(d_model, vocab_size)

    def forward(self, x_tokens):
        batch_size, seq_len = x_tokens.shape
        
        # Komponen 7: Causal Masking
        mask = np.triu(np.full((seq_len, seq_len), -np.inf), k=1)
        
        x = self.token_embedding[x_tokens]
        x = self.pos_encoder(x)
        
        for block in self.decoder_blocks:
            x = block(x, mask)
            
        x = self.ln_final(x)
        logits = x @ self.output_projection
        return logits

# =============================================================================
# 3. Uji Coba Sederhana (Main Execution)
# =============================================================================
if __name__ == "__main__":
    print("===== UJI COBA SEDERHANA TRANSFORMER DARI NOL =====")

    # Definisikan Hyperparameters
    batch_size = 2
    seq_len = 5
    vocab_size = 100
    d_model = 64
    n_heads = 4
    n_layers = 2
    d_ff = 128

    # Buat Input Sederhana (daftar angka)
    input_tokens = np.random.randint(0, vocab_size, size=(batch_size, seq_len))
    print(f"\nBentuk Input Tokens: {input_tokens.shape}")
    print(f"Contoh Input Tokens:\n{input_tokens}\n")

    # Inisialisasi model
    model = GPTDecoder(
        vocab_size=vocab_size,
        d_model=d_model,
        n_layers=n_layers,
        n_heads=n_heads,
        d_ff=d_ff
    )
    
    # Jalankan Forward Pass
    logits = model.forward(input_tokens)

    # Verifikasi Output
    print("--- Verifikasi Output ---")
    print(f"Bentuk Output Logits: {logits.shape}")
    print(f"Ekspektasi Bentuk: [batch_size, seq_len, vocab_size] -> [{batch_size}, {seq_len}, {vocab_size}]\n")
    
    # Ambil probabilitas untuk prediksi token berikutnya
    last_token_logits = logits[:, -1, :]
    next_token_probs = softmax(last_token_logits)
    
    print(f"Bentuk Probabilitas Token Berikutnya: {next_token_probs.shape}")
    
    # Bukti uji bahwa jumlah probabilitas adalah 1
    prob_sum = np.sum(next_token_probs[0])
    print(f"Jumlah probabilitas untuk sampel pertama: {prob_sum:.6f} (seharusnya mendekati 1.0)\n")
    
    # Bukti Causal Mask: cek salah satu matriks attention
    attention_weights_test = model.decoder_blocks[0].mha.attention_weights
    print("Contoh Matriks Perhatian dari Head-0, Sampel-0 (untuk Causal Mask):")
    print("Nilai di atas diagonal utama harusnya mendekati 0.")
    print(np.round(attention_weights_test[0, 0, :, :], 2))
    print("\n===== Uji Coba Selesai =====")